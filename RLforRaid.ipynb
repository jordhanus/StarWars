{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPksFMty53GLctrVKmmWVvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jordhanus/StarWars/blob/main/RLforRaid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cazBnCBSGWo4",
        "outputId": "870eee82-b3a5-444a-aa99-e8978e533c70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SWGoHEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(SWGoHEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(3)  # 0: Basic Attack, 1: Special 1, 2: Special 2\n",
        "        self.observation_space = spaces.Box(low=0, high=100, shape=(6,), dtype=np.int)\n",
        "\n",
        "        # Game state: [Toon1 HP, Toon2 HP, Boss HP, Special1 CD, Special2 CD, Boss Enrage]\n",
        "        self.state = np.array([20, 20, 100, 0, 0, 0])\n",
        "        self.done = False\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        toon1_hp, toon2_hp, boss_hp, special1_cd, special2_cd, boss_enrage = self.state\n",
        "\n",
        "        # Player action\n",
        "        if action == 0:  # Basic Attack\n",
        "            boss_hp -= 2\n",
        "            boss_enrage += 2  # Increase enrage by 2%\n",
        "        elif action == 1 and special1_cd == 0:  # Special 1\n",
        "            boss_hp -= 10\n",
        "            boss_enrage += 2  # Increase enrage by 2% for any damage\n",
        "            special1_cd = 4\n",
        "        elif action == 2 and special2_cd == 0:  # Special 2\n",
        "            toon1_hp = min(20, toon1_hp + 10)\n",
        "            toon2_hp = min(20, toon2_hp + 10)\n",
        "            special2_cd = 4\n",
        "\n",
        "        # Update cooldowns\n",
        "        special1_cd = max(0, special1_cd - 1)\n",
        "        special2_cd = max(0, special2_cd - 1)\n",
        "\n",
        "        # Boss actions (2 turns)\n",
        "        for _ in range(2):\n",
        "            if boss_enrage >= 100:\n",
        "                reward = -100  # Penalty for boss reaching full enrage\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "            if boss_hp <= 0:\n",
        "                reward = 100  # Reward for defeating the boss\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "            # Boss basic attack\n",
        "            target_hp = toon1_hp if toon1_hp < toon2_hp else toon2_hp\n",
        "            target_hp -= 10\n",
        "\n",
        "            # Update the HPs\n",
        "            if toon1_hp < toon2_hp:\n",
        "                toon1_hp = target_hp\n",
        "            else:\n",
        "                toon2_hp = target_hp\n",
        "\n",
        "            # Check for defeat conditions\n",
        "            if toon1_hp <= 0 or toon2_hp <= 0:\n",
        "                reward = -100  # Penalty for losing a toon\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "        self.state = np.array([toon1_hp, toon2_hp, boss_hp, special1_cd, special2_cd, boss_enrage])\n",
        "        return self.state, reward, self.done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([20, 20, 100, 0, 0, 0])\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"State: {self.state}\")\n"
      ],
      "metadata": {
        "id": "lzv9S1q3Gx5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, action_space, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995):\n",
        "        self.action_space = action_space\n",
        "        self.q_table = np.zeros([100, 100, 100, 5, 5, 100, action_space.n])  # Adjust the dimensions based on your state space\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return self.action_space.sample()  # Explore action space\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])  # Exploit learned values\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        old_value = self.q_table[state][action]\n",
        "        next_max = np.max(self.q_table[next_state])\n",
        "\n",
        "        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + self.discount_factor * next_max)\n",
        "        self.q_table[state][action] = new_value\n",
        "\n",
        "        if done:\n",
        "            self.exploration_rate *= self.exploration_decay\n"
      ],
      "metadata": {
        "id": "CBvR9-KiG0ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SWGoHEnv()\n",
        "agent = QLearningAgent(env.action_space)\n",
        "\n",
        "num_episodes = 1000  # Total number of training episodes\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.learn(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode: {episode}, Total Reward: {total_reward}, Exploration Rate: {agent.exploration_rate}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "9TMqmt16HTIX",
        "outputId": "aca0786d-b355-4f43-8a82-5cdc3dd7ccf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-257d013e2393>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  self.observation_space = spaces.Box(low=0, high=100, shape=(6,), dtype=np.int)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-b0094f98fe34>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-55fc168a9d8e>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mold_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mnext_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 0 with size 100"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azYtY-rUHTKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F5qZm7NzHTNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class UpdatedSWGoHEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(UpdatedSWGoHEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(3)  # 0: Basic Attack, 1: Special 1, 2: Special 2\n",
        "        self.observation_space = spaces.Box(low=0, high=100, shape=(6,), dtype=int)\n",
        "\n",
        "        # Game state: [Toon1 HP, Toon2 HP, Boss HP, Special1 CD, Special2 CD, Boss Enrage]\n",
        "        self.state = np.array([50, 50, 50, 0, 0, 0])\n",
        "        self.done = False\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        toon1_hp, toon2_hp, boss_hp, special1_cd, special2_cd, boss_enrage = self.state\n",
        "\n",
        "        # Player action\n",
        "        if action == 0:  # Basic Attack\n",
        "            boss_hp = max(0, boss_hp - 5)\n",
        "            boss_enrage = min(100, boss_enrage + 2)  # Increase enrage by 2%\n",
        "        elif action == 1 and special1_cd == 0:  # Special 1\n",
        "            boss_hp = max(0, boss_hp - 40)\n",
        "            boss_enrage = min(100, boss_enrage + 2)  # Increase enrage by 2% for any damage\n",
        "            special1_cd = 4\n",
        "        elif action == 2 and special2_cd == 0:  # Special 2\n",
        "            toon1_hp = min(20, toon1_hp + 10)\n",
        "            toon2_hp = min(20, toon2_hp + 10)\n",
        "            special2_cd = 2\n",
        "\n",
        "        # Update cooldowns\n",
        "        special1_cd = max(0, special1_cd - 1)\n",
        "        special2_cd = max(0, special2_cd - 1)\n",
        "\n",
        "        # Boss actions (2 turns)\n",
        "        for _ in range(2):\n",
        "            if boss_enrage >= 100:\n",
        "                reward = -100  # Penalty for boss reaching full enrage\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "            if boss_hp <= 0:\n",
        "                reward = 100  # Reward for defeating the boss\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "            # Boss basic attack\n",
        "            target_hp = toon1_hp if toon1_hp < toon2_hp else toon2_hp\n",
        "            target_hp = max(0, target_hp - 10)\n",
        "\n",
        "            # Update the HPs\n",
        "            if toon1_hp < toon2_hp:\n",
        "                toon1_hp = target_hp\n",
        "            else:\n",
        "                toon2_hp = target_hp\n",
        "\n",
        "            # Check for defeat conditions\n",
        "            if toon1_hp <= 0 or toon2_hp <= 0:\n",
        "                reward = -100  # Penalty for losing a toon\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "        self.state = np.array([toon1_hp, toon2_hp, boss_hp, special1_cd, special2_cd, boss_enrage])\n",
        "        return self.state, reward, self.done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([20, 20, 100, 0, 0, 0])\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"State: {self.state}\")\n",
        "\n",
        "def get_state_index(state):\n",
        "    base = [21, 21, 51, 5, 5, 101]\n",
        "    index = 0\n",
        "    multiplier = 1\n",
        "    for s, b in zip(state, base):\n",
        "        index += s * multiplier\n",
        "        multiplier *= b\n",
        "    return index\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, action_space, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995):\n",
        "        self.action_space = action_space\n",
        "        total_states = 21 * 21 * 101 * 5 * 5 * 101  # Based on the state index calculation\n",
        "        self.q_table = np.zeros([total_states, action_space.n])\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state_index = get_state_index(state)\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return self.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state_index])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_index = get_state_index(state)\n",
        "        next_state_index = get_state_index(next_state)\n",
        "        old_value = self.q_table[state_index][action]\n",
        "        next_max = np.max(self.q_table[next_state_index])\n",
        "\n",
        "        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + self.discount_factor * next_max)\n",
        "        self.q_table[state_index][action] = new_value\n",
        "\n",
        "        if done:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "env = UpdatedSWGoHEnv()\n",
        "agent = QLearningAgent(env.action_space)\n",
        "\n",
        "num_episodes = 10000  # Total number of training episodes\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.learn(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    if episode % 1000 == 0:\n",
        "        print(f\"Episode: {episode}, Total Reward: {total_reward}, Exploration Rate: {agent.exploration_rate}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp0BjLSdHTPn",
        "outputId": "8d6cde6a-0fcd-4405-f39c-92b1d544f176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Total Reward: -100, Exploration Rate: 0.995\n",
            "Episode: 1000, Total Reward: -100, Exploration Rate: 0.0066206987359377885\n",
            "Episode: 2000, Total Reward: -100, Exploration Rate: 4.405392135884259e-05\n",
            "Episode: 3000, Total Reward: -100, Exploration Rate: 2.931334084960731e-07\n",
            "Episode: 4000, Total Reward: -100, Exploration Rate: 1.9505004895387866e-09\n",
            "Episode: 5000, Total Reward: -100, Exploration Rate: 1.2978568970387459e-11\n",
            "Episode: 6000, Total Reward: -100, Exploration Rate: 8.635899012716164e-14\n",
            "Episode: 7000, Total Reward: -100, Exploration Rate: 5.746300068057951e-16\n",
            "Episode: 8000, Total Reward: -100, Exploration Rate: 3.823570009739748e-18\n",
            "Episode: 9000, Total Reward: -100, Exploration Rate: 2.5441914703772496e-20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class UpdatedSWGoHEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(UpdatedSWGoHEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(6)  # 3 actions per toon\n",
        "        self.observation_space = spaces.Box(low=0, high=100, shape=(8,), dtype=int)\n",
        "\n",
        "        # State: [Toon1 HP, Toon2 HP, Boss HP, Toon1 Special1 CD, Toon1 Special2 CD, Toon2 Special1 CD, Toon2 Special2 CD, Boss Enrage]\n",
        "        self.state = np.array([20, 20, 100, 0, 0, 0, 0, 0])\n",
        "        self.done = False\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        toon1_hp, toon2_hp, boss_hp, special1_cd, special2_cd, boss_enrage = self.state\n",
        "\n",
        "        # Player action\n",
        "        if action == 0:  # Basic Attack\n",
        "            boss_hp = max(0, boss_hp - 5)\n",
        "            boss_enrage = min(100, boss_enrage + 2)  # Increase enrage by 2%\n",
        "        elif action == 1 and special1_cd == 0:  # Special 1\n",
        "            boss_hp = max(0, boss_hp - 40)\n",
        "            boss_enrage = min(100, boss_enrage + 2)  # Increase enrage by 2% for any damage\n",
        "            special1_cd = 4\n",
        "        elif action == 2 and special2_cd == 0:  # Special 2\n",
        "            toon1_hp = min(20, toon1_hp + 10)\n",
        "            toon2_hp = min(20, toon2_hp + 10)\n",
        "            special2_cd = 2\n",
        "\n",
        "        # Update cooldowns\n",
        "        special1_cd = max(0, special1_cd - 1)\n",
        "        special2_cd = max(0, special2_cd - 1)\n",
        "\n",
        "        # Boss actions (2 turns)\n",
        "        for _ in range(2):\n",
        "            if boss_enrage >= 100:\n",
        "                reward = -100  # Penalty for boss reaching full enrage\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "            if boss_hp <= 0:\n",
        "                reward = 100  # Reward for defeating the boss\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "            # Boss basic attack\n",
        "            target_hp = toon1_hp if toon1_hp < toon2_hp else toon2_hp\n",
        "            target_hp = max(0, target_hp - 10)\n",
        "\n",
        "            # Update the HPs\n",
        "            if toon1_hp < toon2_hp:\n",
        "                toon1_hp = target_hp\n",
        "            else:\n",
        "                toon2_hp = target_hp\n",
        "\n",
        "            # Check for defeat conditions\n",
        "            if toon1_hp <= 0 or toon2_hp <= 0:\n",
        "                reward = -100  # Penalty for losing a toon\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "        self.state = np.array([toon1_hp, toon2_hp, boss_hp, special1_cd, special2_cd, boss_enrage])\n",
        "        return self.state, reward, self.done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([20, 20, 100, 0, 0, 0, 0, 0])\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"State: {self.state}\")\n",
        "\n",
        "def get_state_index(state):\n",
        "    base = [21, 21, 101, 5, 5, 5, 5, 101]\n",
        "    index = 0\n",
        "    multiplier = 1\n",
        "    for s, b in zip(state, base):\n",
        "        index += s * multiplier\n",
        "        multiplier *= b\n",
        "    return index\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, action_space, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995):\n",
        "        self.action_space = action_space\n",
        "        total_states = 21 * 21 * 101 * 5 * 5 * 5 * 5 * 101\n",
        "        self.q_table = np.zeros([total_states, action_space.n])\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state_index = get_state_index(state)\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return self.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state_index])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_index = get_state_index(state)\n",
        "        next_state_index = get_state_index(next_state)\n",
        "        old_value = self.q_table[state_index][action]\n",
        "        next_max = np.max(self.q_table[next_state_index])\n",
        "\n",
        "        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + self.discount_factor * next_max)\n",
        "        self.q_table[state_index][action] = new_value\n",
        "\n",
        "        if done:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "env = UpdatedSWGoHEnv()\n",
        "agent = QLearningAgent(env.action_space)\n",
        "\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.learn(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode: {episode}, Total Reward: {total_reward}, Exploration Rate: {agent.exploration_rate}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "78iqyzMgTAn6",
        "outputId": "d4d82341-e44c-43f8-f499-2c4251519c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-0e8d3c86739e>\u001b[0m in \u001b[0;36m<cell line: 118>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-0e8d3c86739e>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtoon1_hp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoon2_hp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboss_hp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial1_cd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial2_cd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboss_enrage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Player action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 6)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class UpdatedSWGoHEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(UpdatedSWGoHEnv, self).__init__()\n",
        "        self.action_space = spaces.Discrete(6)  # 3 actions per toon\n",
        "        self.observation_space = spaces.Box(low=0, high=100, shape=(8,), dtype=int)\n",
        "\n",
        "        # State: [Toon1 HP, Toon2 HP, Boss HP, Toon1 Special1 CD, Toon1 Special2 CD, Toon2 Special1 CD, Toon2 Special2 CD, Boss Enrage]\n",
        "        self.state = np.array([20, 20, 50, 0, 0, 0, 0, 0])\n",
        "        self.done = False\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        toon1_hp, toon2_hp, boss_hp, t1_s1_cd, t1_s2_cd, t2_s1_cd, t2_s2_cd, boss_enrage = self.state\n",
        "\n",
        "        # Determine which toon is acting and which action is taken\n",
        "        toon_acting = 1 if action < 3 else 2\n",
        "        action = action % 3\n",
        "\n",
        "        # Player action\n",
        "        if action == 0:  # Basic Attack\n",
        "            boss_hp = max(0, boss_hp - 10)\n",
        "            boss_enrage = min(100, boss_enrage + 2)  # Increase enrage by 2%\n",
        "        elif action == 1:  # Special 1\n",
        "            if (toon_acting == 1 and t1_s1_cd == 0) or (toon_acting == 2 and t2_s1_cd == 0):\n",
        "                boss_hp = max(0, boss_hp - 20)\n",
        "                boss_enrage = min(100, boss_enrage + 2)  # Increase enrage by 2%\n",
        "                if toon_acting == 1:\n",
        "                    t1_s1_cd = 4\n",
        "                else:\n",
        "                    t2_s1_cd = 4\n",
        "        elif action == 2:  # Special 2\n",
        "            if (toon_acting == 1 and t1_s2_cd == 0) or (toon_acting == 2 and t2_s2_cd == 0):\n",
        "                toon1_hp = min(20, toon1_hp + 20)\n",
        "                toon2_hp = min(20, toon2_hp + 20)\n",
        "                if toon_acting == 1:\n",
        "                    t1_s2_cd = 4\n",
        "                else:\n",
        "                    t2_s2_cd = 4\n",
        "\n",
        "        # Update cooldowns\n",
        "        t1_s1_cd = max(0, t1_s1_cd - 1)\n",
        "        t1_s2_cd = max(0, t1_s2_cd - 1)\n",
        "        t2_s1_cd = max(0, t2_s1_cd - 1)\n",
        "        t2_s2_cd = max(0, t2_s2_cd - 1)\n",
        "\n",
        "        # Boss actions (2 turns)\n",
        "        for _ in range(2):\n",
        "            if boss_enrage >= 100:\n",
        "                reward = -100  # Penalty for boss reaching full enrage\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "            if boss_hp <= 0:\n",
        "                reward = 100  # Reward for defeating the boss\n",
        "                self.done = True\n",
        "                break\n",
        "\n",
        "            # Boss basic attack\n",
        "            target_hp = toon1_hp if toon1_hp < toon2_hp else toon2_hp\n",
        "            target_hp = max(0, target_hp - 5)\n",
        "\n",
        "            # Update the HPs\n",
        "            if toon1_hp < toon2_hp:\n",
        "                toon1_hp = target_hp\n",
        "            else:\n",
        "                toon2_hp = target_hp\n",
        "\n",
        "            # Check for defeat conditions\n",
        "            if toon1_hp <= 0 or toon2_hp <= 0:\n",
        "                reward = -50  # Penalty for losing a toon\n",
        "                # self.done = True\n",
        "                # break\n",
        "\n",
        "        self.state = np.array([toon1_hp, toon2_hp, boss_hp, t1_s1_cd, t1_s2_cd, t2_s1_cd, t2_s2_cd, boss_enrage])\n",
        "        return self.state, reward, self.done, {}\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([20, 20, 100, 0, 0, 0, 0, 0])\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f\"State: {self.state}\")\n",
        "\n",
        "def get_state_index(state):\n",
        "    base = [21, 21, 101, 5, 5, 5, 5, 101]\n",
        "    index = 0\n",
        "    multiplier = 1\n",
        "    for s, b in zip(state, base):\n",
        "        index += s * multiplier\n",
        "        multiplier *= b\n",
        "    return index\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, action_space, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995):\n",
        "        self.action_space = action_space\n",
        "        total_states = 21 * 21 * 101 * 5 * 5 * 5 * 5 * 101\n",
        "        self.q_table = np.zeros([total_states, action_space.n])\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        state_index = get_state_index(state)\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return self.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state_index])\n",
        "\n",
        "    def learn(self, state, action, reward, next_state, done):\n",
        "        state_index = get_state_index(state)\n",
        "        next_state_index = get_state_index(next_state)\n",
        "        old_value = self.q_table[state_index][action]\n",
        "        next_max = np.max(self.q_table[next_state_index])\n",
        "\n",
        "        new_value = (1 - self.learning_rate) * old_value + self.learning_rate * (reward + self.discount_factor * next_max)\n",
        "        self.q_table[state_index][action] = new_value\n",
        "\n",
        "        if done:\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "\n",
        "env = UpdatedSWGoHEnv()\n",
        "agent = QLearningAgent(env.action_space, learning_rate=0.01, discount_factor=0.99, exploration_rate=2.0, exploration_decay=0.995)\n",
        "\n",
        "best_score = float('-inf')\n",
        "best_actions = []\n",
        "best_episode = 0\n",
        "\n",
        "num_episodes = 10000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    episode_actions = []\n",
        "\n",
        "    while not done:\n",
        "        action = agent.choose_action(state)\n",
        "        episode_actions.append(action)  # Record action\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.learn(state, action, reward, next_state, done)\n",
        "\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    if total_reward > best_score:\n",
        "        best_score = total_reward\n",
        "        best_actions = episode_actions\n",
        "        best_episode = episode\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode: {episode}, Total Reward: {total_reward}, Exploration Rate: {agent.exploration_rate}\")\n",
        "\n",
        "print(f\"Best Episode: {best_episode}, Best Score: {best_score}, Actions: {best_actions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W98L0ZDRTAqO",
        "outputId": "08535639-4604-4987-82be-1d6afc1028e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Total Reward: -400, Exploration Rate: 1.99\n",
            "Episode: 100, Total Reward: -350, Exploration Rate: 1.2054831686165484\n",
            "Episode: 200, Total Reward: -300, Exploration Rate: 0.7302460652350725\n",
            "Episode: 300, Total Reward: -150, Exploration Rate: 0.4423614776830866\n",
            "Episode: 400, Total Reward: -200, Exploration Rate: 0.2679695054227667\n",
            "Episode: 500, Total Reward: -200, Exploration Rate: 0.1623280042661538\n",
            "Episode: 600, Total Reward: -250, Exploration Rate: 0.09833350599897663\n",
            "Episode: 700, Total Reward: -150, Exploration Rate: 0.05956753085066369\n",
            "Episode: 800, Total Reward: -250, Exploration Rate: 0.036084249164081414\n",
            "Episode: 900, Total Reward: -300, Exploration Rate: 0.021858771366565784\n",
            "Episode: 1000, Total Reward: -300, Exploration Rate: 0.013241397471875577\n",
            "Episode: 1100, Total Reward: -550, Exploration Rate: 0.008021247126285285\n",
            "Episode: 1200, Total Reward: -150, Exploration Rate: 0.004859034372889833\n",
            "Episode: 1300, Total Reward: -300, Exploration Rate: 0.002943459372988924\n",
            "Episode: 1400, Total Reward: -300, Exploration Rate: 0.0017830606691682277\n",
            "Episode: 1500, Total Reward: -250, Exploration Rate: 0.0010801254398514863\n",
            "Episode: 1600, Total Reward: -200, Exploration Rate: 0.0006543080591635745\n",
            "Episode: 1700, Total Reward: -350, Exploration Rate: 0.00039636047859891983\n",
            "Episode: 1800, Total Reward: -250, Exploration Rate: 0.0002401034601285414\n",
            "Episode: 1900, Total Reward: -200, Exploration Rate: 0.0001454475778450006\n",
            "Episode: 2000, Total Reward: -450, Exploration Rate: 8.810784271768517e-05\n",
            "Episode: 2100, Total Reward: -200, Exploration Rate: 5.337312634134853e-05\n",
            "Episode: 2200, Total Reward: -150, Exploration Rate: 3.2331862040673455e-05\n",
            "Episode: 2300, Total Reward: -350, Exploration Rate: 1.9585686180936764e-05\n",
            "Episode: 2400, Total Reward: -300, Exploration Rate: 1.1864429666796486e-05\n",
            "Episode: 2500, Total Reward: -400, Exploration Rate: 7.18712073796885e-06\n",
            "Episode: 2600, Total Reward: -150, Exploration Rate: 4.353745266550952e-06\n",
            "Episode: 2700, Total Reward: -150, Exploration Rate: 2.6373701704880108e-06\n",
            "Episode: 2800, Total Reward: -100, Exploration Rate: 1.5976408793641483e-06\n",
            "Episode: 2900, Total Reward: -300, Exploration Rate: 9.678036128478508e-07\n",
            "Episode: 3000, Total Reward: -300, Exploration Rate: 5.862668169921462e-07\n",
            "Episode: 3100, Total Reward: -250, Exploration Rate: 3.551431056293621e-07\n",
            "Episode: 3200, Total Reward: -700, Exploration Rate: 2.151351941137714e-07\n",
            "Episode: 3300, Total Reward: -450, Exploration Rate: 1.303225404428168e-07\n",
            "Episode: 3400, Total Reward: -200, Exploration Rate: 7.894554220862573e-08\n",
            "Episode: 3500, Total Reward: -500, Exploration Rate: 4.78228755627164e-08\n",
            "Episode: 3600, Total Reward: -250, Exploration Rate: 2.896968420386848e-08\n",
            "Episode: 3700, Total Reward: -400, Exploration Rate: 1.754897824517595e-08\n",
            "Episode: 3800, Total Reward: -150, Exploration Rate: 1.0630652211546524e-08\n",
            "Episode: 3900, Total Reward: -350, Exploration Rate: 6.439734830369662e-09\n",
            "Episode: 4000, Total Reward: -250, Exploration Rate: 3.901000979077573e-09\n",
            "Episode: 4100, Total Reward: -200, Exploration Rate: 2.3631110658465803e-09\n",
            "Episode: 4200, Total Reward: -200, Exploration Rate: 1.4315028218339523e-09\n",
            "Episode: 4300, Total Reward: -250, Exploration Rate: 8.671620892200625e-10\n",
            "Episode: 4400, Total Reward: -150, Exploration Rate: 5.253011572950486e-10\n",
            "Episode: 4500, Total Reward: -150, Exploration Rate: 3.182119113437063e-10\n",
            "Episode: 4600, Total Reward: -150, Exploration Rate: 1.9276336843122574e-10\n",
            "Episode: 4700, Total Reward: -250, Exploration Rate: 1.1677034983400665e-10\n",
            "Episode: 4800, Total Reward: -200, Exploration Rate: 7.073602578812123e-11\n",
            "Episode: 4900, Total Reward: -150, Exploration Rate: 4.284979321728957e-11\n",
            "Episode: 5000, Total Reward: -450, Exploration Rate: 2.5957137940774917e-11\n",
            "Episode: 5100, Total Reward: -250, Exploration Rate: 1.5724066780433263e-11\n",
            "Episode: 5200, Total Reward: -300, Exploration Rate: 9.525174796992415e-12\n",
            "Episode: 5300, Total Reward: -100, Exploration Rate: 5.7700692944245776e-12\n",
            "Episode: 5400, Total Reward: -500, Exploration Rate: 3.495337395065324e-12\n",
            "Episode: 5500, Total Reward: -150, Exploration Rate: 2.1173720594910848e-12\n",
            "Episode: 5600, Total Reward: -200, Exploration Rate: 1.2826413966911861e-12\n",
            "Episode: 5700, Total Reward: -200, Exploration Rate: 7.769862387346962e-13\n",
            "Episode: 5800, Total Reward: -100, Exploration Rate: 4.706752929856058e-13\n",
            "Episode: 5900, Total Reward: -250, Exploration Rate: 2.8512117767729165e-13\n",
            "Episode: 6000, Total Reward: -250, Exploration Rate: 1.7271798025432328e-13\n",
            "Episode: 6100, Total Reward: -400, Exploration Rate: 1.0462744628845831e-13\n",
            "Episode: 6200, Total Reward: -200, Exploration Rate: 6.338021380706955e-14\n",
            "Episode: 6300, Total Reward: -150, Exploration Rate: 3.839385978278421e-14\n",
            "Episode: 6400, Total Reward: -200, Exploration Rate: 2.3257865199180998e-14\n",
            "Episode: 6500, Total Reward: -350, Exploration Rate: 1.4088927153550397e-14\n",
            "Episode: 6600, Total Reward: -250, Exploration Rate: 8.534655551492297e-15\n",
            "Episode: 6700, Total Reward: -150, Exploration Rate: 5.170042018725503e-15\n",
            "Episode: 6800, Total Reward: -100, Exploration Rate: 3.1318586103587544e-15\n",
            "Episode: 6900, Total Reward: -200, Exploration Rate: 1.897187357424269e-15\n",
            "Episode: 7000, Total Reward: -200, Exploration Rate: 1.1492600136115903e-15\n",
            "Episode: 7100, Total Reward: -200, Exploration Rate: 6.961877400868328e-16\n",
            "Episode: 7200, Total Reward: -300, Exploration Rate: 4.217299511918939e-16\n",
            "Episode: 7300, Total Reward: -200, Exploration Rate: 2.5547153661472694e-16\n",
            "Episode: 7400, Total Reward: -150, Exploration Rate: 1.5475710424606007e-16\n",
            "Episode: 7500, Total Reward: -200, Exploration Rate: 9.374727858917689e-17\n",
            "Episode: 7600, Total Reward: -100, Exploration Rate: 5.678932987078355e-17\n",
            "Episode: 7700, Total Reward: -300, Exploration Rate: 3.4401297143840496e-17\n",
            "Episode: 7800, Total Reward: -150, Exploration Rate: 2.083928878667148e-17\n",
            "Episode: 7900, Total Reward: -200, Exploration Rate: 1.2623825064458306e-17\n",
            "Episode: 8000, Total Reward: -150, Exploration Rate: 7.647140019479495e-18\n",
            "Episode: 8100, Total Reward: -200, Exploration Rate: 4.632411347505807e-18\n",
            "Episode: 8200, Total Reward: -150, Exploration Rate: 2.8061778439831917e-18\n",
            "Episode: 8300, Total Reward: -150, Exploration Rate: 1.699899577420309e-18\n",
            "Episode: 8400, Total Reward: -200, Exploration Rate: 1.029748909004305e-18\n",
            "Episode: 8500, Total Reward: -100, Exploration Rate: 6.237914460833889e-19\n",
            "Episode: 8600, Total Reward: -400, Exploration Rate: 3.7787441657311658e-19\n",
            "Episode: 8700, Total Reward: -200, Exploration Rate: 2.2890515026617594e-19\n",
            "Episode: 8800, Total Reward: -100, Exploration Rate: 1.3866397279171707e-19\n",
            "Episode: 8900, Total Reward: -150, Exploration Rate: 8.399853532357687e-20\n",
            "Episode: 9000, Total Reward: -200, Exploration Rate: 5.088382940754499e-20\n",
            "Episode: 9100, Total Reward: -150, Exploration Rate: 3.082391955052825e-20\n",
            "Episode: 9200, Total Reward: -150, Exploration Rate: 1.8672219200478572e-20\n",
            "Episode: 9300, Total Reward: -300, Exploration Rate: 1.131107837532445e-20\n",
            "Episode: 9400, Total Reward: -150, Exploration Rate: 6.851916884601122e-21\n",
            "Episode: 9500, Total Reward: -200, Exploration Rate: 4.150688681983015e-21\n",
            "Episode: 9600, Total Reward: -150, Exploration Rate: 2.514364494621973e-21\n",
            "Episode: 9700, Total Reward: -200, Exploration Rate: 1.523127677403941e-21\n",
            "Episode: 9800, Total Reward: -100, Exploration Rate: 9.226657179720939e-22\n",
            "Episode: 9900, Total Reward: -250, Exploration Rate: 5.589236147109866e-22\n",
            "Best Episode: 2317, Best Score: -50, Actions: [1, 5, 2, 4, 1, 5, 2, 4, 1]\n"
          ]
        }
      ]
    }
  ]
}